{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_processing(trainfile, num):\n",
    "    # Read the training data from CSV file\n",
    "    # File has no column names and values are separated by spaces\n",
    "    df = pd.read_csv(trainfile, sep=\" \", header=None)\n",
    "    \n",
    "    # Drop columns 26 and 27 from the dataframe\n",
    "    # They don't contain relevant information and many values are NANs\n",
    "    df = df.drop([26, 27], axis=1)\n",
    "    \n",
    "    # Rename the columns:\n",
    "    # - first two columns are \"ID\" and \"Cycle\"\n",
    "    # - next three columns represent operational settings: \"OpSet1\", \"OpSet2\", and \"OpSet3\"\n",
    "    # - remaining columns (from 4 to 25) represent sensor measurements: \"SensorMeasure1\" to \"SensorMeasure21\"\n",
    "    df.columns = [\"ID\", \"Cycle\"] + [f\"OpSet{i}\" for i in range(1, 4)] + [f\"SensorMeasure{i}\" for i in range(1, 22)]\n",
    "    \n",
    "    # Calculate end-of-life (EOL) cycle for each engine (each unique \"ID\")\n",
    "    # The 'EOL' column contains the maximum cycle number for each engine\n",
    "    df['EOL'] = df.groupby('ID')['Cycle'].transform('max')\n",
    "    \n",
    "    # Calculate Remaining Useful Life (RUL) for each cycle by subtracting current cycle num from the EOL\n",
    "    # This gives the number of cycles remaining before the engine reaches the end of its life\n",
    "    df['RUL'] = df['EOL'] - df['Cycle']\n",
    "    \n",
    "    # Add 1000 times the number of the current dataset to the IDs\n",
    "    # This is to distinguish between engines from different datasets\n",
    "    df['ID'] = df['ID'].apply(lambda x: int(num * 1000 + x))\n",
    "    \n",
    "    # Drop 'EOL' column since it's no longer needed and return processed dataframe\n",
    "    return df.drop(columns=['EOL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_processing(testfile, rulfile, num):\n",
    "    # Read the testing data from CSV file\n",
    "    # File has no column names and values are separated by spaces\n",
    "    df = pd.read_csv(testfile, sep=\" \", header=None)\n",
    "    \n",
    "    # Drop columns 26 and 27 from the dataframe\n",
    "    # They don't contain relevant information and many values are NANs\n",
    "    df = df.drop([26, 27], axis=1)\n",
    "    \n",
    "    # Rename the columns:\n",
    "    # - The first two columns are \"ID\" and \"Cycle\"\n",
    "    # - The next three columns represent operational settings: \"OpSet1\", \"OpSet2\", and \"OpSet3\"\n",
    "    # - The remaining columns (from 4 to 25) represent sensor measurements: \"SensorMeasure1\" to \"SensorMeasure21\"\n",
    "    df.columns = [\"ID\", \"Cycle\"] + [f\"OpSet{i}\" for i in range(1, 4)] + [f\"SensorMeasure{i}\" for i in range(1, 22)]\n",
    "    \n",
    "    # Read the Remaining Useful Life (RUL) values from the parameter RUL file\n",
    "    rul_list = pd.read_csv(rulfile, header=None)[0].values\n",
    "    \n",
    "    # Calculate the total number of cycles for each engine in the test data\n",
    "    df['NumCycles'] = df.groupby('ID')['Cycle'].transform('count')\n",
    "    \n",
    "    # Calculate the RUL for each engine in the test data:\n",
    "    # - The RUL for each cycle is the sum of the RUL from the RUL file and the difference between the total cycle count and the current cycle number\n",
    "    df['RUL'] = rul_list[df['ID'].astype(int) - 1] + df['NumCycles'] - df['Cycle']\n",
    "    \n",
    "    # Adjust the 'ID' values by adding 1000 times the dataset number\n",
    "    # This is to ensure unique IDs across different datasets\n",
    "    df['ID'] = df['ID'].apply(lambda x: int(num * 1000 + x))\n",
    "    \n",
    "    # Drop the 'NumCycles' column since it's no longer needed and return processed dataframe\n",
    "    return df.drop(columns=['NumCycles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames for training and testing data\n",
    "training_set, testing_set = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# Loop through each of the four datasets (FD001 to FD004)\n",
    "for num in range(1, 5):\n",
    "    # Process the training/testing data from the current dataset and concatenate it to the overall training/testing set\n",
    "    training_set = pd.concat([training_set, training_processing(f\"cmapss\\\\train_FD00{num}.txt\", num)], ignore_index=True)\n",
    "    testing_set = pd.concat([testing_set, testing_processing(f\"cmapss\\\\test_FD00{num}.txt\", f\"cmapss\\\\RUL_FD00{num}.txt\", num)], ignore_index=True)\n",
    "\n",
    "# Separate the 'ID' and 'RUL' columns from the rest of the training and testing sets\n",
    "# 'ID' and 'RUL' are not needed for feature scaling and will be added back later\n",
    "trainingIDs, testingIDs = training_set.pop('ID'), testing_set.pop('ID')\n",
    "trainingRULs, testingRULs = training_set.pop('RUL'), testing_set.pop('RUL')\n",
    "\n",
    "# Store the original column names for later use after scaling\n",
    "original_columns = training_set.columns\n",
    "\n",
    "# Initialize a MinMaxScaler to scale the data (sensor measurements and operational settings)\n",
    "data_scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the training data and convert it back to a DataFrame with the original column names\n",
    "training_set = pd.DataFrame(data_scaler.fit_transform(training_set), columns=original_columns)\n",
    "\n",
    "# Scale the testing data using the same scaler (fit on training data) and convert it back to a DataFrame\n",
    "testing_set = pd.DataFrame(data_scaler.transform(testing_set), columns=original_columns)\n",
    "\n",
    "# Initialize a MinMaxScaler to scale the RUL values separately\n",
    "rul_scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the training RULs and reshape them to a 2D array required by the scaler\n",
    "training_rul_scaled = pd.DataFrame(rul_scaler.fit_transform(trainingRULs.values.reshape(-1, 1)))\n",
    "\n",
    "# Scale the testing RULs using the same scaler (fit on training RULs) and reshape to a DataFrame\n",
    "testing_rul_scaled = pd.DataFrame(rul_scaler.transform(testingRULs.values.reshape(-1, 1)))\n",
    "\n",
    "# Concatenate the 'ID' column, scaled feature data, and scaled RULs back together for the training/testing sets\n",
    "training_set = pd.concat([trainingIDs, training_set, training_rul_scaled], axis=1)\n",
    "testing_set = pd.concat([testingIDs, testing_set, testing_rul_scaled], axis=1)\n",
    "\n",
    "# Save the processed training/testing set to CSV files in the \"processed\" directory\n",
    "training_set.to_csv(f\"processed\\\\training_set\", index=False)\n",
    "testing_set.to_csv(f\"processed\\\\testing_set\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padded_sequences(data_set):\n",
    "    # Group the dataset by 'ID', keeping only the feature columns (excluding 'ID')\n",
    "    grouped = data_set.groupby('ID')[list(data_set.columns)[1:]]\n",
    "    \n",
    "    # Initialize an empty list to store sequences for each engine (each unique 'ID')\n",
    "    sequences = []\n",
    "    \n",
    "    # Iterate over each group (each engine's data)\n",
    "    for _, group in grouped:\n",
    "        # Convert the group's data to a list of lists (each sublist represents a row)\n",
    "        sequence = group.values.tolist()\n",
    "        \n",
    "        # Append the sequence (all cycles for one engine) to the sequences list\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    # Determine the maximum length of the sequences (i.e., the engine with the most cycles)\n",
    "    # This length will be used to pad (add 0s) all sequences to the same length\n",
    "    #! maxlen for training_set is 543 and maxlen for testing_set is 486.\n",
    "    #! Because the model is not capable of handling a variable maxlen, a constant number is necessary.\n",
    "    #! 545 was chosen because it's greater than both maxlens and for simplicity.\n",
    "    maxlen = 545\n",
    "    \n",
    "    # Pad all sequences to the maximum length (maxlen)\n",
    "    # Sequences shorter than maxlen will be padded with zeros at the end ('post' padding)\n",
    "    # Sequences longer than maxlen will be truncated to maxlen ('post' truncating)\n",
    "    # Truncating will not happen since every sequence is shorter than chosen maxlen\n",
    "    # All elements are cast to 'float32' type\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post', dtype='float32')\n",
    "    \n",
    "    # Return the padded sequences as a NumPy array\n",
    "    return padded_sequences\n",
    "\n",
    "# Create padded sequences for the training/testing set\n",
    "train_sequences = create_padded_sequences(training_set)\n",
    "test_sequences = create_padded_sequences(testing_set)\n",
    "\n",
    "\n",
    "# Save the training/testing padded_sequences to CSV files in the \"processed\" directory\n",
    "with open(f\"processed\\\\train_padded_sequences.txt\", 'w') as f:\n",
    "    for sequence in train_sequences:\n",
    "        for row in sequence:\n",
    "            f.write(' '.join(map(str, row)) + '\\n')\n",
    "        f.write('\\n')\n",
    "with open(f\"processed\\\\test_padded_sequences.txt\", 'w') as f:\n",
    "    for sequence in test_sequences:\n",
    "        for row in sequence:\n",
    "            f.write(' '.join(map(str, row)) + '\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (X) and labels (y) from the training sequences\n",
    "# The features (X) include all columns except the last one\n",
    "# The last column is the target variable (RUL), which becomes the label (y)\n",
    "x_train = train_sequences[:, :, :-1]\n",
    "y_train = train_sequences[:, :, -1]\n",
    "\n",
    "# Extract features (X) and labels (y) from the testing sequences\n",
    "# The features (X) include all columns except the last one\n",
    "# The last column is the target variable (RUL), which becomes the label (y)\n",
    "x_test = test_sequences[:, :, :-1]\n",
    "y_test = test_sequences[:, :, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Masking, LSTM, Bidirectional, Dense, Dropout, BatchNormalization\n",
    "\n",
    "class LSTMModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.3, mask_value=0.0):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # Masking layer to ignore padded values (zeros) in the input sequences\n",
    "        # `mask_value` specifies the value that should be ignored during training\n",
    "        self.masking = Masking(mask_value=mask_value)\n",
    "\n",
    "        # First LSTM layer: processes the input sequences and returns the full sequence\n",
    "        # `hidden_dim` specifies the number of units in the LSTM\n",
    "        # `return_sequences=True` ensures that the LSTM returns the entire sequence of outputs\n",
    "        self.lstm1 = Bidirectional(LSTM(hidden_dim, return_sequences=True))\n",
    "\n",
    "        # Second LSTM layer: processes the output of the first LSTM and returns the final output only\n",
    "        # `return_sequences=False` ensures that only the last output of the sequence is returned\n",
    "        self.lstm2 = Bidirectional(LSTM(hidden_dim, return_sequences=False))\n",
    "\n",
    "        # Dropout layers: used to prevent overfitting by randomly setting a fraction of input units to 0 during training\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "        # Batch normalization layers: normalize the output of the LSTM layers to improve training stability\n",
    "        # `training=training` ensures that batch normalization behaves differently during training and inference\n",
    "        self.batch_norm1 = BatchNormalization()\n",
    "        self.batch_norm2 = BatchNormalization()\n",
    "\n",
    "        # Dense layer 1: fully connected layer with ReLU activation\n",
    "        # `hidden_dim` is the number of output units\n",
    "        # `kernel_regularizer` adds L2 regularization to prevent overfitting\n",
    "        # L2 regularization helps prevent overfitting by adding a penalty proportional to the square of the weights' magnitude.\n",
    "        self.dense1 = Dense(hidden_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "\n",
    "        # Dense layer 2: another fully connected layer with ReLU activation\n",
    "        # This layer reduces the dimensionality by half\n",
    "        self.dense2 = Dense(hidden_dim // 2, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "\n",
    "        # Output layer: final dense layer that outputs the prediction\n",
    "        # `output_dim` specifies the number of output units\n",
    "        # `activation='linear'` which is used for regression tasks, produces continuous values\n",
    "        self.output_layer = Dense(output_dim, activation='linear')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Apply masking to ignore padded values\n",
    "        x = self.masking(inputs)\n",
    "\n",
    "        # Pass the masked input through the first LSTM layer\n",
    "        x = self.lstm1(x)\n",
    "\n",
    "        # Apply dropout and batch normalization to the output of the first LSTM layer\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.batch_norm1(x, training=training)\n",
    "\n",
    "        # Pass the normalized output through the second LSTM layer\n",
    "        x = self.lstm2(x)\n",
    "\n",
    "        # Apply dropout and batch normalization to the output of the second LSTM layer\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.batch_norm2(x, training=training)\n",
    "\n",
    "        # Pass the normalized output through the first dense layer\n",
    "        x = self.dense1(x)\n",
    "\n",
    "        # Pass the output through the second dense layer\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        # Return the final output from the output layer\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 10s/step - loss: 2.6825 - mae: 0.2363 - mse: 0.0982 - val_loss: 2.2367 - val_mae: 0.1167 - val_mse: 0.0400 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 11s/step - loss: 2.1488 - mae: 0.1257 - mse: 0.0299 - val_loss: 1.8490 - val_mae: 0.1133 - val_mse: 0.0381 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 12s/step - loss: 1.7459 - mae: 0.0933 - mse: 0.0186 - val_loss: 1.5177 - val_mae: 0.1092 - val_mse: 0.0362 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 12s/step - loss: 1.4162 - mae: 0.0772 - mse: 0.0136 - val_loss: 1.2411 - val_mae: 0.1049 - val_mse: 0.0343 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 12s/step - loss: 1.1428 - mae: 0.0636 - mse: 0.0093 - val_loss: 1.0136 - val_mae: 0.1004 - val_mse: 0.0317 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 12s/step - loss: 0.9209 - mae: 0.0540 - mse: 0.0075 - val_loss: 0.8260 - val_mae: 0.0939 - val_mse: 0.0273 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 13s/step - loss: 0.7388 - mae: 0.0443 - mse: 0.0054 - val_loss: 0.6740 - val_mae: 0.0884 - val_mse: 0.0244 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14s/step - loss: 0.5936 - mae: 0.0382 - mse: 0.0042 - val_loss: 0.5571 - val_mae: 0.0892 - val_mse: 0.0245 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 13s/step - loss: 0.4738 - mae: 0.0302 - mse: 0.0026 - val_loss: 0.4538 - val_mae: 0.0803 - val_mse: 0.0205 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 13s/step - loss: 0.3806 - mae: 0.0267 - mse: 0.0022 - val_loss: 0.3625 - val_mae: 0.0642 - val_mse: 0.0140 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 13s/step - loss: 0.3064 - mae: 0.0239 - mse: 0.0019 - val_loss: 0.3147 - val_mae: 0.0774 - val_mse: 0.0192 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 13s/step - loss: 0.2479 - mae: 0.0228 - mse: 0.0017 - val_loss: 0.2666 - val_mae: 0.0774 - val_mse: 0.0185 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 13s/step - loss: 0.1981 - mae: 0.0187 - mse: 0.0012 - val_loss: 0.2212 - val_mae: 0.0699 - val_mse: 0.0156 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 12s/step - loss: 0.1635 - mae: 0.0202 - mse: 0.0014 - val_loss: 0.1832 - val_mae: 0.0625 - val_mse: 0.0138 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 12s/step - loss: 0.1319 - mae: 0.0174 - mse: 0.0011 - val_loss: 0.1629 - val_mae: 0.0663 - val_mse: 0.0150 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 13s/step - loss: 0.1088 - mae: 0.0170 - mse: 9.5612e-04 - val_loss: 0.1327 - val_mae: 0.0551 - val_mse: 0.0109 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 13s/step - loss: 0.0914 - mae: 0.0180 - mse: 0.0012 - val_loss: 0.1089 - val_mae: 0.0462 - val_mse: 0.0081 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 13s/step - loss: 0.0791 - mae: 0.0194 - mse: 0.0013 - val_loss: 0.1079 - val_mae: 0.0572 - val_mse: 0.0112 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 12s/step - loss: 0.0636 - mae: 0.0154 - mse: 8.0368e-04 - val_loss: 0.0942 - val_mae: 0.0529 - val_mse: 0.0101 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 13s/step - loss: 0.0518 - mae: 0.0125 - mse: 5.8611e-04 - val_loss: 0.0831 - val_mae: 0.0493 - val_mse: 0.0088 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 13s/step - loss: 0.0449 - mae: 0.0128 - mse: 6.3502e-04 - val_loss: 0.0798 - val_mae: 0.0522 - val_mse: 0.0100 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 12s/step - loss: 0.0400 - mae: 0.0136 - mse: 7.2157e-04 - val_loss: 0.0703 - val_mae: 0.0477 - val_mse: 0.0088 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14s/step - loss: 0.0381 - mae: 0.0164 - mse: 0.0013 - val_loss: 0.0781 - val_mae: 0.0594 - val_mse: 0.0113 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 14s/step - loss: 0.0348 - mae: 0.0168 - mse: 0.0013 - val_loss: 0.0614 - val_mae: 0.0453 - val_mse: 0.0078 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 14s/step - loss: 0.0287 - mae: 0.0134 - mse: 7.2304e-04 - val_loss: 0.0535 - val_mae: 0.0399 - val_mse: 0.0060 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 14s/step - loss: 0.0244 - mae: 0.0113 - mse: 5.6269e-04 - val_loss: 0.0539 - val_mae: 0.0423 - val_mse: 0.0065 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 13s/step - loss: 0.0226 - mae: 0.0114 - mse: 6.2822e-04 - val_loss: 0.0610 - val_mae: 0.0515 - val_mse: 0.0091 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 15s/step - loss: 0.0236 - mae: 0.0142 - mse: 8.7464e-04 - val_loss: 0.0425 - val_mae: 0.0339 - val_mse: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 14s/step - loss: 0.0204 - mae: 0.0123 - mse: 7.1287e-04 - val_loss: 0.0495 - val_mae: 0.0416 - val_mse: 0.0060 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 15s/step - loss: 0.0205 - mae: 0.0129 - mse: 7.9887e-04 - val_loss: 0.0600 - val_mae: 0.0535 - val_mse: 0.0094 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 15s/step - loss: 0.0182 - mae: 0.0116 - mse: 6.8307e-04 - val_loss: 0.0595 - val_mae: 0.0537 - val_mse: 0.0101 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 15s/step - loss: 0.0189 - mae: 0.0131 - mse: 9.0123e-04 - val_loss: 0.0483 - val_mae: 0.0426 - val_mse: 0.0059 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 13s/step - loss: 0.0167 - mae: 0.0113 - mse: 6.2589e-04 - val_loss: 0.0398 - val_mae: 0.0348 - val_mse: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 13s/step - loss: 0.0167 - mae: 0.0117 - mse: 5.8260e-04 - val_loss: 0.0522 - val_mae: 0.0477 - val_mse: 0.0081 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 13s/step - loss: 0.0147 - mae: 0.0102 - mse: 5.3043e-04 - val_loss: 0.0240 - val_mae: 0.0192 - val_mse: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 14s/step - loss: 0.0164 - mae: 0.0118 - mse: 6.7622e-04 - val_loss: 0.0434 - val_mae: 0.0395 - val_mse: 0.0059 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 13s/step - loss: 0.0161 - mae: 0.0119 - mse: 6.4309e-04 - val_loss: 0.0446 - val_mae: 0.0409 - val_mse: 0.0063 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 13s/step - loss: 0.0172 - mae: 0.0136 - mse: 8.9886e-04 - val_loss: 0.0331 - val_mae: 0.0291 - val_mse: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 12s/step - loss: 0.0144 - mae: 0.0105 - mse: 5.5418e-04 - val_loss: 0.0525 - val_mae: 0.0492 - val_mse: 0.0078 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 12s/step - loss: 0.0140 - mae: 0.0106 - mse: 5.3225e-04 - val_loss: 0.0335 - val_mae: 0.0299 - val_mse: 0.0036 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 13s/step - loss: 0.0142 - mae: 0.0107 - mse: 4.9667e-04 - val_loss: 0.0351 - val_mae: 0.0317 - val_mse: 0.0036 - learning_rate: 2.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 13s/step - loss: 0.0115 - mae: 0.0081 - mse: 4.0759e-04 - val_loss: 0.0321 - val_mae: 0.0287 - val_mse: 0.0029 - learning_rate: 2.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 13s/step - loss: 0.0130 - mae: 0.0095 - mse: 3.9944e-04 - val_loss: 0.0307 - val_mae: 0.0272 - val_mse: 0.0028 - learning_rate: 2.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 13s/step - loss: 0.0137 - mae: 0.0103 - mse: 5.0068e-04 - val_loss: 0.0374 - val_mae: 0.0342 - val_mse: 0.0039 - learning_rate: 2.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 13s/step - loss: 0.0125 - mae: 0.0092 - mse: 4.9616e-04 - val_loss: 0.0305 - val_mae: 0.0271 - val_mse: 0.0026 - learning_rate: 2.0000e-04\n",
      "23/23 - 9s - 385ms/step - loss: 0.0371 - mae: 0.0323 - mse: 0.0048\n",
      "Test Loss: 0.0371\n",
      "Test MAE: 0.0323\n",
      "Test MSE: 0.0048\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = x_train.shape[2]  # The number of features in the input data\n",
    "hidden_dim = 128  # Number of units in the LSTM layers (hidden state dimensionality)\n",
    "output_dim = 545  # Output dimensionality (RUL prediction for each cycle)\n",
    "learning_rate = 0.001  # Initial learning rate for the optimizer\n",
    "batch_size = 64  # Number of samples per gradient update\n",
    "epochs = 50  # Number of epochs to train the model\n",
    "\n",
    "# Model instantiation\n",
    "# Initialize the LSTM model with the specified input, hidden, and output dimensions\n",
    "model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "# Compile the model\n",
    "# Adam optimizer is used with the specified learning rate\n",
    "# The loss function is mean absolute error (MAE), and we also track MAE and MSE as metrics\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mae', metrics=['mae', 'mse'])\n",
    "\n",
    "# Callbacks\n",
    "# EarlyStopping: Stop training if the validation loss doesn't improve for 10 epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# ReduceLROnPlateau: Reduce learning rate by a factor of 0.2 if validation loss doesn't improve for 5 epochs, with a minimum learning rate of 1e-6\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Save the model only if the validation loss improves; keeps best version of model, helps in selecting a model that generalizes well to unseen data\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Train the model\n",
    "# Fit the model on the training data, using 20% of it for validation\n",
    "# The training process will stop early if validation loss doesn't improve (early_stopping)\n",
    "# Learning rate will be reduced if the model stops improving (reduce_lr)\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,  # Number of samples processed before the model's internal parameters are updated\n",
    "    epochs=epochs,  # Number of complete passes through the training data\n",
    "    validation_split=0.2,  # Fraction of training data used for validation\n",
    "    callbacks=[early_stopping, reduce_lr, model_checkpoint],  # List of callbacks to apply during training\n",
    "    verbose=1  # Verbosity mode (1 = progress bar)\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"Test Loss: {evaluation[0]:.4f}\")  # Print the test loss (MAE)\n",
    "print(f\"Test MAE: {evaluation[1]:.4f}\")   # Print the test MAE\n",
    "print(f\"Test MSE: {evaluation[2]:.4f}\")   # Print the test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original and Scaled MAE: 17.52151468396187, 0.03232751786708832\n"
     ]
    }
   ],
   "source": [
    "mae_scaled = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "\n",
    "# To convert MAE back to original scale:\n",
    "mae_original = rul_scaler.inverse_transform(np.array([[mae_scaled]]))[0][0]\n",
    "\n",
    "print(f\"Original and Scaled MAE: {mae_original}, {mae_scaled}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
